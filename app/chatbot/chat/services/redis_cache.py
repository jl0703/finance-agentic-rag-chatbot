import logging
import time
from typing import Any, Dict, Optional

from app.core.config_setup import SEMANTIC_CACHE

logger = logging.getLogger(__name__)

class RedisCache:
    def __init__(self):
        self.cache = SEMANTIC_CACHE
        
    def _detect_intention(self, query: str) -> str:
        """
        Detects the intention of the query for caching.

        Args:
            query (str): The user query.

        Returns:
            str: The detected intention.
        """
        q = query.lower()
        
        if any(word in q for word in ["recommend", "should i", "buy", "sell", "is it good to", "advice"]):
            return "recommendation"
        
        if any(word in q for word in ["service", "provide", "offer", "what does", "revenue"]):
            return "services"
        
        if any(tok in q for tok in ["what is", "who is", "overview", "introduce"]):
            return "overview"
        
        if any(tok in q for tok in ["trend", "forecast", "future", "outlook", "analysis"]):
            return "analysis"

        return "general"

    def get_cached(self, query: str) -> Optional[Dict[str, Any]]:
        """
        Returns a cached response if similar embedding exists within the 
        distance threshold.

        Args:
            query (str): The user query to search in the cache.

        Returns:
            Optional[Dict[str, Any]]: The cached response if found, else None.
        """
        intent = self._detect_intention(query)
        
        try:
            result = self.cache.check(
                prompt=query,
                num_results=1,
                return_fields=["prompt", "response", "metadata"]
            )
        except Exception as e:
            logger.error("Semantic cache check error: %s", str(e))
            return None
        
        if not result:
            return None
        
        hit = result[0]
        metadata = hit.get("metadata", {})
        hit_intent = metadata.get("intent")
        
        if hit_intent and hit_intent != intent:
            logger.debug("Cache intent mismatch.")
            return None
        
        logger.info("Cache hit")
        
        return hit
    
    def store(
        self, 
        query: str, 
        response: str, 
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Stores the query-response pair in the cache.

        Args:
            query (str): The user query.
            response (str): The response generated by the LLM.
            metadata (Optional[Dict[str, Any]], optional): Additional metadata to
                store with the response. Defaults to None.

        Returns:
            str: The ID of the cached response.
        """
        metadata = metadata or {}
        metadata["intent"] = self._detect_intention(query)
        metadata["stored_at"] = int(time.time())
        
        try:
            key = self.cache.store(prompt=query, response=response, metadata=metadata)
        except Exception as e:
            logger.error("Error storing to cache: %s", str(e))
            raise
        
        logger.info("Stored response in cache with key: %s", key)
        
        return key
    
    def clear_all(self) -> None:
        """
        Clears all entries in the cache.
        """
        try:
            self.cache.clear()
        except Exception as e:
            logger.error("Error clearing cache: %s", str(e))